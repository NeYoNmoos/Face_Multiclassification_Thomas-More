{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in face count and names for ../data/images/cleaned_images/0195.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0601.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0703.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0387.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0710.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0235.jpg. Skipping...\n",
      "No faces detected in ../data/images/cleaned_images/0151.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0281.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 1/46 [00:15<11:19, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces detected in ../data/images/cleaned_images/0155.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0290.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0190.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0626.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0367.jpg. Skipping...\n",
      "No faces detected in ../data/images/cleaned_images/0042.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0616.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0600.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0545.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 2/46 [00:30<11:07, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in face count and names for ../data/images/cleaned_images/0389.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0424.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0477.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0261.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0399.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0475.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0623.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0338.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0738.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0468.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 3/46 [00:45<10:56, 15.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in face count and names for ../data/images/cleaned_images/0342.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0345.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0364.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0751.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0102.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0380.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0147.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▊         | 4/46 [01:00<10:41, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in face count and names for ../data/images/cleaned_images/0238.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0415.jpg. Skipping...\n",
      "No faces detected in ../data/images/cleaned_images/0069.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0724.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0303.jpg. Skipping...\n",
      "No faces detected in ../data/images/cleaned_images/0829.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0663.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0263.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0609.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0288.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0718.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0502.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0523.jpg. Skipping...\n",
      "Mismatch in face count and names for ../data/images/cleaned_images/0714.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 5/46 [01:18<10:44, 15.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/faces5_train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/labels/clean_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 139\u001b[0m \u001b[43mpreprocess_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 131\u001b[0m, in \u001b[0;36mpreprocess_faces\u001b[0;34m(dataset_dir, output_dir, csv_path, batch_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if not in CSV\u001b[39;00m\n\u001b[1;32m    130\u001b[0m names \u001b[38;5;241m=\u001b[39m names_mapping[img_name]\n\u001b[0;32m--> 131\u001b[0m \u001b[43mdetect_and_save_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 59\u001b[0m, in \u001b[0;36mdetect_and_save_faces\u001b[0;34m(file_path, output_dir, names, target_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Detect faces with RetinaFace\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m faces:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo faces detected in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:123\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m    121\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m im_tensor, im_info, im_scale \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39mpreprocess_image(img, allow_upscaling)\n\u001b[0;32m--> 123\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m net_out \u001b[38;5;241m=\u001b[39m [elt\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m net_out]\n\u001b[1;32m    125\u001b[0m sym_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from retinaface import RetinaFace\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def align_face(image, landmarks, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Align face using eye landmarks and crop to the bounding box.\n",
    "    Args:\n",
    "        image (numpy array): Input image.\n",
    "        landmarks (dict): Detected landmarks with 'left_eye' and 'right_eye'.\n",
    "        target_size (tuple): Desired output size for the aligned face.\n",
    "    Returns:\n",
    "        numpy array: Aligned and resized face.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        left_eye = np.array(landmarks[\"left_eye\"])\n",
    "        right_eye = np.array(landmarks[\"right_eye\"])\n",
    "\n",
    "        # Compute angle between eyes\n",
    "        delta_y = right_eye[1] - left_eye[1]\n",
    "        delta_x = right_eye[0] - left_eye[0]\n",
    "        angle = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "        # Compute center between eyes\n",
    "        eye_center = (left_eye + right_eye) / 2\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(tuple(eye_center), angle, scale=1)\n",
    "\n",
    "        # Rotate the entire image\n",
    "        rotated_img = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        return rotated_img\n",
    "    except Exception as e:\n",
    "        print(f\"Error aligning face: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def detect_and_save_faces(file_path, output_dir, names, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Detect faces, align using landmarks, and save them in folders named after people.\n",
    "    Args:\n",
    "        file_path (str): Path to the image.\n",
    "        output_dir (str): Path to save processed images.\n",
    "        names (list): List of names corresponding to detected faces.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image\n",
    "        img = cv2.imread(file_path)\n",
    "        if img is None:\n",
    "            print(f\"Unable to read {file_path}. Skipping...\")\n",
    "            return\n",
    "\n",
    "        # Detect faces with RetinaFace\n",
    "        faces = RetinaFace.detect_faces(file_path)\n",
    "\n",
    "        if not faces:\n",
    "            print(f\"No faces detected in {file_path}. Skipping...\")\n",
    "            return\n",
    "\n",
    "        # Sort detected faces by their x-coordinates\n",
    "        sorted_faces = sorted(faces.items(), key=lambda item: item[1][\"facial_area\"][0])\n",
    "\n",
    "        # Ensure face count matches the number of names\n",
    "        if len(sorted_faces) != len(names):\n",
    "            print(f\"Mismatch in face count and names for {file_path}. Skipping...\")\n",
    "            return\n",
    "\n",
    "        # Process and save each face\n",
    "        for name, (key, face_data) in zip(names, sorted_faces):\n",
    "            landmarks = face_data[\"landmarks\"]\n",
    "            aligned_face = align_face(img, landmarks, target_size)\n",
    "\n",
    "            if aligned_face is None:\n",
    "                continue\n",
    "\n",
    "            # Crop the face bounding box\n",
    "            x1, y1, x2, y2 = face_data[\"facial_area\"]\n",
    "            cropped_face = aligned_face[y1:y2, x1:x2]\n",
    "\n",
    "            # Resize to target size\n",
    "            resized_face = cv2.resize(cropped_face, target_size)\n",
    "\n",
    "            # Save aligned face\n",
    "            person_dir = os.path.join(output_dir, name)\n",
    "            os.makedirs(person_dir, exist_ok=True)\n",
    "            output_file_path = os.path.join(person_dir, os.path.basename(file_path))\n",
    "            Image.fromarray(cv2.cvtColor(resized_face, cv2.COLOR_BGR2RGB)).save(output_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def preprocess_faces(dataset_dir, output_dir, csv_path, batch_size=16):\n",
    "    \"\"\"\n",
    "    Preprocess dataset images by detecting, aligning, and saving faces.\n",
    "    Args:\n",
    "        dataset_dir (str): Path to the dataset.\n",
    "        output_dir (str): Path to save processed images.\n",
    "        csv_path (str): Path to the CSV file with image-to-names mapping.\n",
    "        batch_size (int): Number of images to process per batch.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load names mapping from CSV\n",
    "    names_data = pd.read_csv(csv_path)\n",
    "    names_mapping = {\n",
    "        str(row[\"image\"]): row[\"label_name\"].split(\";\")\n",
    "        for _, row in names_data.iterrows()\n",
    "    }\n",
    "\n",
    "    # Collect image paths\n",
    "    image_files = [os.path.join(root, file)\n",
    "                   for root, _, files in os.walk(dataset_dir)\n",
    "                   for file in files if file.endswith(\".jpg\")]\n",
    "\n",
    "    # Process images in batches\n",
    "    for i in tqdm(range(0, len(image_files), batch_size), desc=\"Processing batches\"):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        for file_path in batch_files:\n",
    "            # Extract image name without leading zeros\n",
    "            img_name = os.path.basename(file_path).split('.')[0].lstrip(\"0\")\n",
    "            if img_name not in names_mapping:\n",
    "                continue  # Skip if not in CSV\n",
    "\n",
    "            names = names_mapping[img_name]\n",
    "            detect_and_save_faces(file_path, output_dir, names)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_dir = \"../data/images/cleaned_images\"\n",
    "output_dir = \"../data/faces5_train\"\n",
    "csv_path = \"../data/labels/clean_data.csv\"\n",
    "\n",
    "preprocess_faces(dataset_dir, output_dir, csv_path, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_folder, label_map=None):\n",
    "    images = []\n",
    "    image_labels = []\n",
    "\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            images.append((filename, img))\n",
    "            if label_map:\n",
    "                image_labels.append(label_map.get(filename, []))  # Default to empty list for test images\n",
    "\n",
    "    return images, image_labels if label_map else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0032.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0032.jpg. Expected 1, found 0.\n",
      "Processing 0195.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9996499419212341), 'facial_area': [np.int64(282), np.int64(142), np.int64(407), np.int64(311)], 'landmarks': {'right_eye': [np.float32(310.58563), np.float32(210.92842)], 'left_eye': [np.float32(370.72183), np.float32(210.68129)], 'nose': [np.float32(338.374), np.float32(243.73334)], 'mouth_right': [np.float32(317.30133), np.float32(270.83136)], 'mouth_left': [np.float32(365.88617), np.float32(271.02014)]}}, 'face_2': {'score': np.float64(0.9977530837059021), 'facial_area': [np.int64(169), np.int64(241), np.int64(186), np.int64(259)], 'landmarks': {'right_eye': [np.float32(172.85423), np.float32(249.6075)], 'left_eye': [np.float32(180.10976), np.float32(249.28572)], 'nose': [np.float32(175.98828), np.float32(254.03227)], 'mouth_right': [np.float32(174.8612), np.float32(256.62875)], 'mouth_left': [np.float32(179.8528), np.float32(256.29584)]}}}\n",
      "Warning: Face count mismatch for 0195.jpg. Expected 2, found 0.\n",
      "Processing 0569.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9988412261009216), 'facial_area': [np.int64(157), np.int64(132), np.int64(353), np.int64(320)], 'landmarks': {'right_eye': [np.float32(210.39084), np.float32(212.01765)], 'left_eye': [np.float32(308.8397), np.float32(203.92332)], 'nose': [np.float32(263.02484), np.float32(239.42458)], 'mouth_right': [np.float32(219.58905), np.float32(278.04004)], 'mouth_left': [np.float32(301.95114), np.float32(270.02707)]}}}\n",
      "Warning: Face count mismatch for 0569.jpg. Expected 1, found 0.\n",
      "Processing 0601.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9955841302871704), 'facial_area': [np.int64(152), np.int64(106), np.int64(175), np.int64(131)], 'landmarks': {'right_eye': [np.float32(155.91957), np.float32(116.052124)], 'left_eye': [np.float32(162.54407), np.float32(116.155655)], 'nose': [np.float32(155.6418), np.float32(120.47973)], 'mouth_right': [np.float32(156.5555), np.float32(125.477394)], 'mouth_left': [np.float32(161.13686), np.float32(125.35375)]}}, 'face_2': {'score': np.float64(0.9663454294204712), 'facial_area': [np.int64(378), np.int64(96), np.int64(423), np.int64(132)], 'landmarks': {'right_eye': [np.float32(402.01868), np.float32(111.44843)], 'left_eye': [np.float32(415.08774), np.float32(112.29629)], 'nose': [np.float32(410.00906), np.float32(121.891)], 'mouth_right': [np.float32(397.84717), np.float32(124.0702)], 'mouth_left': [np.float32(406.73868), np.float32(125.08871)]}}}\n",
      "Warning: Face count mismatch for 0601.jpg. Expected 2, found 0.\n",
      "Processing 0072.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9977648854255676), 'facial_area': [np.int64(360), np.int64(178), np.int64(421), np.int64(231)], 'landmarks': {'right_eye': [np.float32(373.69733), np.float32(201.21075)], 'left_eye': [np.float32(401.4302), np.float32(199.67276)], 'nose': [np.float32(386.9843), np.float32(213.38867)], 'mouth_right': [np.float32(379.1018), np.float32(220.19937)], 'mouth_left': [np.float32(399.8697), np.float32(218.58258)]}}}\n",
      "Warning: Face count mismatch for 0072.jpg. Expected 1, found 0.\n",
      "Processing 0703.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9997928142547607), 'facial_area': [np.int64(180), np.int64(106), np.int64(417), np.int64(350)], 'landmarks': {'right_eye': [np.float32(245.04283), np.float32(217.40718)], 'left_eye': [np.float32(356.32672), np.float32(219.63576)], 'nose': [np.float32(301.05356), np.float32(266.39948)], 'mouth_right': [np.float32(246.57501), np.float32(289.23898)], 'mouth_left': [np.float32(350.91733), np.float32(289.9603)]}}, 'face_2': {'score': np.float64(0.9901672005653381), 'facial_area': [np.int64(86), np.int64(216), np.int64(120), np.int64(253)], 'landmarks': {'right_eye': [np.float32(96.57276), np.float32(228.98335)], 'left_eye': [np.float32(111.042885), np.float32(227.85991)], 'nose': [np.float32(105.31536), np.float32(235.3713)], 'mouth_right': [np.float32(100.16851), np.float32(243.51782)], 'mouth_left': [np.float32(110.719185), np.float32(242.67807)]}}}\n",
      "Warning: Face count mismatch for 0703.jpg. Expected 2, found 0.\n",
      "Processing 0516.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9997596740722656), 'facial_area': [np.int64(144), np.int64(279), np.int64(410), np.int64(479)], 'landmarks': {'right_eye': [np.float32(211.19595), np.float32(398.9616)], 'left_eye': [np.float32(333.90213), np.float32(386.5357)], 'nose': [np.float32(276.5078), np.float32(445.15207)], 'mouth_right': [np.float32(230.3246), np.float32(508.274)], 'mouth_left': [np.float32(335.53424), np.float32(497.10913)]}}}\n",
      "Warning: Face count mismatch for 0516.jpg. Expected 2, found 0.\n",
      "Processing 0387.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9994445443153381), 'facial_area': [np.int64(285), np.int64(179), np.int64(357), np.int64(269)], 'landmarks': {'right_eye': [np.float32(300.99506), np.float32(217.71936)], 'left_eye': [np.float32(333.9804), np.float32(213.18565)], 'nose': [np.float32(317.66736), np.float32(231.10878)], 'mouth_right': [np.float32(307.3497), np.float32(246.98955)], 'mouth_left': [np.float32(336.94516), np.float32(242.94135)]}}, 'face_2': {'score': np.float64(0.9989173412322998), 'facial_area': [np.int64(185), np.int64(235), np.int64(214), np.int64(277)], 'landmarks': {'right_eye': [np.float32(190.43042), np.float32(249.87329)], 'left_eye': [np.float32(194.7507), np.float32(250.64645)], 'nose': [np.float32(186.97661), np.float32(258.62534)], 'mouth_right': [np.float32(192.94629), np.float32(266.07626)], 'mouth_left': [np.float32(196.41267), np.float32(266.20584)]}}}\n",
      "Warning: Face count mismatch for 0387.jpg. Expected 2, found 0.\n",
      "Processing 0710.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9997038841247559), 'facial_area': [np.int64(371), np.int64(190), np.int64(513), np.int64(346)], 'landmarks': {'right_eye': [np.float32(396.62283), np.float32(270.79312)], 'left_eye': [np.float32(448.01242), np.float32(236.11372)], 'nose': [np.float32(429.464), np.float32(269.59464)], 'mouth_right': [np.float32(433.7375), np.float32(317.52353)], 'mouth_left': [np.float32(477.3976), np.float32(289.69122)]}}, 'face_2': {'score': np.float64(0.9992977976799011), 'facial_area': [np.int64(259), np.int64(172), np.int64(355), np.int64(308)], 'landmarks': {'right_eye': [np.float32(284.30945), np.float32(226.28166)], 'left_eye': [np.float32(330.58936), np.float32(223.76967)], 'nose': [np.float32(309.24826), np.float32(248.51425)], 'mouth_right': [np.float32(292.37256), np.float32(276.02637)], 'mouth_left': [np.float32(328.91736), np.float32(274.09296)]}}, 'face_3': {'score': np.float64(0.998987078666687), 'facial_area': [np.int64(48), np.int64(167), np.int64(164), np.int64(305)], 'landmarks': {'right_eye': [np.float32(86.43316), np.float32(219.10918)], 'left_eye': [np.float32(136.70578), np.float32(232.61)], 'nose': [np.float32(104.77987), np.float32(242.9891)], 'mouth_right': [np.float32(74.39505), np.float32(265.1154)], 'mouth_left': [np.float32(115.6539), np.float32(276.91766)]}}}\n",
      "Warning: Face count mismatch for 0710.jpg. Expected 3, found 0.\n",
      "Processing 0356.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9991486072540283), 'facial_area': [np.int64(256), np.int64(142), np.int64(334), np.int64(246)], 'landmarks': {'right_eye': [np.float32(275.19308), np.float32(185.64879)], 'left_eye': [np.float32(311.66495), np.float32(181.21556)], 'nose': [np.float32(294.04068), np.float32(196.96992)], 'mouth_right': [np.float32(281.0758), np.float32(219.8403)], 'mouth_left': [np.float32(311.61945), np.float32(216.44937)]}}}\n",
      "Warning: Face count mismatch for 0356.jpg. Expected 1, found 0.\n",
      "Processing 0463.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9955664873123169), 'facial_area': [np.int64(333), np.int64(160), np.int64(401), np.int64(217)], 'landmarks': {'right_eye': [np.float32(350.75076), np.float32(183.16812)], 'left_eye': [np.float32(383.2236), np.float32(183.72913)], 'nose': [np.float32(366.61172), np.float32(193.42235)], 'mouth_right': [np.float32(353.7945), np.float32(203.5785)], 'mouth_left': [np.float32(378.6682), np.float32(203.98611)]}}}\n",
      "Warning: Face count mismatch for 0463.jpg. Expected 1, found 0.\n",
      "Processing 0325.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9871885776519775), 'facial_area': [np.int64(233), np.int64(177), np.int64(330), np.int64(267)], 'landmarks': {'right_eye': [np.float32(262.1748), np.float32(214.22343)], 'left_eye': [np.float32(306.25427), np.float32(209.01361)], 'nose': [np.float32(288.25247), np.float32(227.3732)], 'mouth_right': [np.float32(271.82794), np.float32(246.75183)], 'mouth_left': [np.float32(305.78906), np.float32(242.77805)]}}}\n",
      "Warning: Face count mismatch for 0325.jpg. Expected 1, found 0.\n",
      "Processing 0235.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9971935153007507), 'facial_area': [np.int64(309), np.int64(147), np.int64(368), np.int64(194)], 'landmarks': {'right_eye': [np.float32(326.74936), np.float32(163.9977)], 'left_eye': [np.float32(354.1069), np.float32(164.83298)], 'nose': [np.float32(342.28284), np.float32(171.21597)], 'mouth_right': [np.float32(330.54886), np.float32(180.59474)], 'mouth_left': [np.float32(352.6868), np.float32(181.23996)]}}, 'face_2': {'score': np.float64(0.996009886264801), 'facial_area': [np.int64(508), np.int64(143), np.int64(573), np.int64(199)], 'landmarks': {'right_eye': [np.float32(530.53925), np.float32(165.64264)], 'left_eye': [np.float32(560.9419), np.float32(165.29355)], 'nose': [np.float32(548.82056), np.float32(173.83252)], 'mouth_right': [np.float32(532.9815), np.float32(185.10054)], 'mouth_left': [np.float32(558.57764), np.float32(184.57681)]}}, 'face_3': {'score': np.float64(0.9959138631820679), 'facial_area': [np.int64(35), np.int64(158), np.int64(96), np.int64(210)], 'landmarks': {'right_eye': [np.float32(56.423645), np.float32(176.93567)], 'left_eye': [np.float32(84.91275), np.float32(176.02585)], 'nose': [np.float32(75.319855), np.float32(185.89597)], 'mouth_right': [np.float32(61.233536), np.float32(195.80797)], 'mouth_left': [np.float32(86.12505), np.float32(195.02054)]}}}\n",
      "Warning: Face count mismatch for 0235.jpg. Expected 3, found 0.\n",
      "Processing 0151.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0151.jpg. Expected 4, found 0.\n",
      "Processing 0281.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9972633123397827), 'facial_area': [np.int64(168), np.int64(166), np.int64(251), np.int64(239)], 'landmarks': {'right_eye': [np.float32(204.86212), np.float32(193.80511)], 'left_eye': [np.float32(240.6507), np.float32(196.68044)], 'nose': [np.float32(227.12021), np.float32(212.37398)], 'mouth_right': [np.float32(200.46304), np.float32(220.13684)], 'mouth_left': [np.float32(229.82872), np.float32(222.86247)]}}, 'face_2': {'score': np.float64(0.9939119815826416), 'facial_area': [np.int64(323), np.int64(216), np.int64(383), np.int64(275)], 'landmarks': {'right_eye': [np.float32(344.62424), np.float32(240.00171)], 'left_eye': [np.float32(371.91693), np.float32(243.13513)], 'nose': [np.float32(359.76904), np.float32(250.84132)], 'mouth_right': [np.float32(345.44257), np.float32(259.96863)], 'mouth_left': [np.float32(366.6019), np.float32(262.29578)]}}}\n",
      "Warning: Face count mismatch for 0281.jpg. Expected 2, found 0.\n",
      "Processing 0158.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9996945261955261), 'facial_area': [np.int64(275), np.int64(135), np.int64(365), np.int64(254)], 'landmarks': {'right_eye': [np.float32(299.68533), np.float32(184.68752)], 'left_eye': [np.float32(341.6081), np.float32(181.25597)], 'nose': [np.float32(322.2997), np.float32(201.2499)], 'mouth_right': [np.float32(302.50916), np.float32(221.68503)], 'mouth_left': [np.float32(342.4243), np.float32(218.73605)]}}}\n",
      "Warning: Face count mismatch for 0158.jpg. Expected 1, found 0.\n",
      "Processing 0159.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9998854994773865), 'facial_area': [np.int64(230), np.int64(130), np.int64(446), np.int64(358)], 'landmarks': {'right_eye': [np.float32(292.3416), np.float32(225.52788)], 'left_eye': [np.float32(391.73962), np.float32(215.70966)], 'nose': [np.float32(346.2753), np.float32(262.87982)], 'mouth_right': [np.float32(300.1868), np.float32(301.8054)], 'mouth_left': [np.float32(388.33984), np.float32(292.64102)]}}}\n",
      "Warning: Face count mismatch for 0159.jpg. Expected 1, found 0.\n",
      "Processing 0155.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0155.jpg. Expected 2, found 0.\n",
      "Processing 0290.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9958151578903198), 'facial_area': [np.int64(53), np.int64(153), np.int64(200), np.int64(287)], 'landmarks': {'right_eye': [np.float32(109.30215), np.float32(208.47752)], 'left_eye': [np.float32(176.37445), np.float32(205.97322)], 'nose': [np.float32(154.8352), np.float32(231.13374)], 'mouth_right': [np.float32(116.101715), np.float32(256.03415)], 'mouth_left': [np.float32(171.118), np.float32(254.04898)]}}, 'face_2': {'score': np.float64(0.9881319403648376), 'facial_area': [np.int64(253), np.int64(194), np.int64(334), np.int64(262)], 'landmarks': {'right_eye': [np.float32(276.53345), np.float32(219.85794)], 'left_eye': [np.float32(317.45337), np.float32(219.03998)], 'nose': [np.float32(298.12823), np.float32(233.37479)], 'mouth_right': [np.float32(279.0976), np.float32(246.0728)], 'mouth_left': [np.float32(313.11353), np.float32(245.21912)]}}}\n",
      "Warning: Face count mismatch for 0290.jpg. Expected 2, found 0.\n",
      "Processing 0135.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0135.jpg. Expected 2, found 0.\n",
      "Processing 0190.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9995418787002563), 'facial_area': [np.int64(339), np.int64(185), np.int64(392), np.int64(252)], 'landmarks': {'right_eye': [np.float32(354.14908), np.float32(211.3103)], 'left_eye': [np.float32(379.04138), np.float32(214.91467)], 'nose': [np.float32(364.48853), np.float32(228.59003)], 'mouth_right': [np.float32(352.77646), np.float32(234.45474)], 'mouth_left': [np.float32(373.4348), np.float32(237.4656)]}}, 'face_2': {'score': np.float64(0.9993516206741333), 'facial_area': [np.int64(170), np.int64(176), np.int64(214), np.int64(238)], 'landmarks': {'right_eye': [np.float32(174.5127), np.float32(200.8419)], 'left_eye': [np.float32(188.09837), np.float32(201.3119)], 'nose': [np.float32(172.42883), np.float32(210.79619)], 'mouth_right': [np.float32(175.84888), np.float32(222.30722)], 'mouth_left': [np.float32(187.40266), np.float32(222.0934)]}}, 'face_3': {'score': np.float64(0.9985091090202332), 'facial_area': [np.int64(459), np.int64(235), np.int64(509), np.int64(296)], 'landmarks': {'right_eye': [np.float32(472.21158), np.float32(261.89114)], 'left_eye': [np.float32(495.81097), np.float32(264.27322)], 'nose': [np.float32(481.83945), np.float32(275.68796)], 'mouth_right': [np.float32(472.25134), np.float32(282.60168)], 'mouth_left': [np.float32(490.63983), np.float32(284.4376)]}}}\n",
      "Warning: Face count mismatch for 0190.jpg. Expected 4, found 0.\n",
      "Processing 0626.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9991670846939087), 'facial_area': [np.int64(168), np.int64(119), np.int64(206), np.int64(154)], 'landmarks': {'right_eye': [np.float32(174.92128), np.float32(129.39062)], 'left_eye': [np.float32(191.05106), np.float32(131.24)], 'nose': [np.float32(179.04298), np.float32(137.1437)], 'mouth_right': [np.float32(174.46089), np.float32(142.74384)], 'mouth_left': [np.float32(188.00502), np.float32(144.1332)]}}, 'face_2': {'score': np.float64(0.997472882270813), 'facial_area': [np.int64(111), np.int64(147), np.int64(156), np.int64(212)], 'landmarks': {'right_eye': [np.float32(147.78758), np.float32(172.2493)], 'left_eye': [np.float32(147.96996), np.float32(172.91963)], 'nose': [np.float32(157.04173), np.float32(182.07726)], 'mouth_right': [np.float32(147.69339), np.float32(195.04332)], 'mouth_left': [np.float32(148.04796), np.float32(196.12303)]}}, 'face_3': {'score': np.float64(0.9959802031517029), 'facial_area': [np.int64(413), np.int64(156), np.int64(453), np.int64(192)], 'landmarks': {'right_eye': [np.float32(422.46368), np.float32(167.94592)], 'left_eye': [np.float32(436.1407), np.float32(171.5974)], 'nose': [np.float32(422.00327), np.float32(174.32576)], 'mouth_right': [np.float32(417.05197), np.float32(180.19249)], 'mouth_left': [np.float32(427.33517), np.float32(182.9112)]}}, 'face_4': {'score': np.float64(0.9955800175666809), 'facial_area': [np.int64(563), np.int64(175), np.int64(616), np.int64(228)], 'landmarks': {'right_eye': [np.float32(574.29333), np.float32(194.55032)], 'left_eye': [np.float32(591.97845), np.float32(199.73036)], 'nose': [np.float32(571.71844), np.float32(203.3489)], 'mouth_right': [np.float32(567.2119), np.float32(214.38876)], 'mouth_left': [np.float32(579.88934), np.float32(217.96982)]}}, 'face_5': {'score': np.float64(0.9897103905677795), 'facial_area': [np.int64(251), np.int64(138), np.int64(273), np.int64(163)], 'landmarks': {'right_eye': [np.float32(264.86768), np.float32(147.23233)], 'left_eye': [np.float32(271.0302), np.float32(147.85007)], 'nose': [np.float32(271.0055), np.float32(152.0008)], 'mouth_right': [np.float32(264.288), np.float32(156.46318)], 'mouth_left': [np.float32(268.88904), np.float32(157.26045)]}}, 'face_6': {'score': np.float64(0.9186830520629883), 'facial_area': [np.int64(302), np.int64(157), np.int64(336), np.int64(189)], 'landmarks': {'right_eye': [np.float32(308.01117), np.float32(168.31873)], 'left_eye': [np.float32(321.8576), np.float32(169.75839)], 'nose': [np.float32(311.18405), np.float32(174.91988)], 'mouth_right': [np.float32(308.24826), np.float32(180.95876)], 'mouth_left': [np.float32(318.1877), np.float32(181.93256)]}}}\n",
      "Warning: Face count mismatch for 0626.jpg. Expected 5, found 0.\n",
      "Processing 0367.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9995843768119812), 'facial_area': [np.int64(341), np.int64(72), np.int64(464), np.int64(223)], 'landmarks': {'right_eye': [np.float32(360.76697), np.float32(140.86281)], 'left_eye': [np.float32(409.30005), np.float32(118.572495)], 'nose': [np.float32(385.41092), np.float32(152.37976)], 'mouth_right': [np.float32(384.0564), np.float32(189.68887)], 'mouth_left': [np.float32(426.63855), np.float32(171.64961)]}}, 'face_2': {'score': np.float64(0.9993488788604736), 'facial_area': [np.int64(0), np.int64(125), np.int64(79), np.int64(247)], 'landmarks': {'right_eye': [np.float32(0.9862053), np.float32(174.41661)], 'left_eye': [np.float32(36.372948), np.float32(173.58278)], 'nose': [np.float32(8.316725), np.float32(194.22334)], 'mouth_right': [np.float32(3.3572993), np.float32(211.43045)], 'mouth_left': [np.float32(38.854786), np.float32(210.63387)]}}, 'face_3': {'score': np.float64(0.9990975260734558), 'facial_area': [np.int64(179), np.int64(64), np.int64(299), np.int64(237)], 'landmarks': {'right_eye': [np.float32(211.27518), np.float32(133.67557)], 'left_eye': [np.float32(269.28326), np.float32(132.47423)], 'nose': [np.float32(241.56798), np.float32(165.41232)], 'mouth_right': [np.float32(217.43477), np.float32(195.20366)], 'mouth_left': [np.float32(264.1289), np.float32(194.2596)]}}}\n",
      "Warning: Face count mismatch for 0367.jpg. Expected 3, found 0.\n",
      "Processing 0042.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0042.jpg. Expected 1, found 0.\n",
      "Processing 0130.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0130.jpg. Expected 1, found 0.\n",
      "Processing 0068.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9786950945854187), 'facial_area': [np.int64(202), np.int64(72), np.int64(314), np.int64(170)], 'landmarks': {'right_eye': [np.float32(219.99356), np.float32(111.574135)], 'left_eye': [np.float32(265.0849), np.float32(106.90622)], 'nose': [np.float32(235.22705), np.float32(130.01204)], 'mouth_right': [np.float32(233.09293), np.float32(149.611)], 'mouth_left': [np.float32(265.67648), np.float32(145.67355)]}}}\n",
      "Warning: Face count mismatch for 0068.jpg. Expected 1, found 0.\n",
      "Processing 0046.jpg...\n",
      "Detections: {}\n",
      "Warning: Face count mismatch for 0046.jpg. Expected 1, found 0.\n",
      "Processing 0616.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9686823487281799), 'facial_area': [np.int64(337), np.int64(157), np.int64(357), np.int64(176)], 'landmarks': {'right_eye': [np.float32(342.68118), np.float32(164.75175)], 'left_eye': [np.float32(352.05243), np.float32(165.01006)], 'nose': [np.float32(347.07886), np.float32(169.7245)], 'mouth_right': [np.float32(343.83008), np.float32(172.96016)], 'mouth_left': [np.float32(350.35522), np.float32(173.17418)]}}}\n",
      "Warning: Face count mismatch for 0616.jpg. Expected 2, found 0.\n",
      "Processing 0600.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9979370832443237), 'facial_area': [np.int64(183), np.int64(156), np.int64(219), np.int64(190)], 'landmarks': {'right_eye': [np.float32(189.52281), np.float32(168.38417)], 'left_eye': [np.float32(204.1064), np.float32(169.60326)], 'nose': [np.float32(191.88683), np.float32(173.73598)], 'mouth_right': [np.float32(188.26213), np.float32(181.13177)], 'mouth_left': [np.float32(199.12671), np.float32(182.01509)]}}, 'face_2': {'score': np.float64(0.9975900650024414), 'facial_area': [np.int64(269), np.int64(111), np.int64(340), np.int64(172)], 'landmarks': {'right_eye': [np.float32(284.80756), np.float32(136.81856)], 'left_eye': [np.float32(314.96954), np.float32(133.21704)], 'nose': [np.float32(299.11148), np.float32(142.89098)], 'mouth_right': [np.float32(290.72037), np.float32(157.41965)], 'mouth_left': [np.float32(314.6681), np.float32(154.41661)]}}}\n",
      "Warning: Face count mismatch for 0600.jpg. Expected 2, found 0.\n",
      "Processing 0545.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9867774844169617), 'facial_area': [np.int64(407), np.int64(173), np.int64(478), np.int64(242)], 'landmarks': {'right_eye': [np.float32(416.02945), np.float32(196.24284)], 'left_eye': [np.float32(442.22845), np.float32(196.11827)], 'nose': [np.float32(418.9434), np.float32(207.39105)], 'mouth_right': [np.float32(414.4978), np.float32(225.03369)], 'mouth_left': [np.float32(433.36664), np.float32(224.65987)]}}}\n",
      "Warning: Face count mismatch for 0545.jpg. Expected 4, found 0.\n",
      "Processing 0497.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9992861151695251), 'facial_area': [np.int64(207), np.int64(47), np.int64(453), np.int64(397)], 'landmarks': {'right_eye': [np.float32(267.45428), np.float32(184.95355)], 'left_eye': [np.float32(384.03104), np.float32(185.05673)], 'nose': [np.float32(320.4474), np.float32(270.83105)], 'mouth_right': [np.float32(270.76266), np.float32(303.58484)], 'mouth_left': [np.float32(379.49817), np.float32(303.07477)]}}}\n",
      "Warning: Face count mismatch for 0497.jpg. Expected 1, found 0.\n",
      "Processing 0389.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9959973096847534), 'facial_area': [np.int64(346), np.int64(149), np.int64(439), np.int64(241)], 'landmarks': {'right_eye': [np.float32(364.64075), np.float32(188.60751)], 'left_eye': [np.float32(404.75095), np.float32(181.88773)], 'nose': [np.float32(383.33588), np.float32(203.32506)], 'mouth_right': [np.float32(374.99277), np.float32(219.83742)], 'mouth_left': [np.float32(406.24475), np.float32(214.23396)]}}, 'face_2': {'score': np.float64(0.9706881642341614), 'facial_area': [np.int64(152), np.int64(178), np.int64(226), np.int64(240)], 'landmarks': {'right_eye': [np.float32(168.35524), np.float32(203.13228)], 'left_eye': [np.float32(203.83469), np.float32(200.15208)], 'nose': [np.float32(186.47939), np.float32(212.59798)], 'mouth_right': [np.float32(175.90721), np.float32(225.92534)], 'mouth_left': [np.float32(203.41524), np.float32(223.3256)]}}}\n",
      "Warning: Face count mismatch for 0389.jpg. Expected 2, found 0.\n",
      "Processing 0424.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9985016584396362), 'facial_area': [np.int64(20), np.int64(186), np.int64(43), np.int64(212)], 'landmarks': {'right_eye': [np.float32(28.307076), np.float32(195.03882)], 'left_eye': [np.float32(38.59142), np.float32(195.38461)], 'nose': [np.float32(34.222424), np.float32(200.27872)], 'mouth_right': [np.float32(28.949738), np.float32(204.8626)], 'mouth_left': [np.float32(37.501247), np.float32(205.13875)]}}, 'face_2': {'score': np.float64(0.9982559084892273), 'facial_area': [np.int64(132), np.int64(200), np.int64(145), np.int64(217)], 'landmarks': {'right_eye': [np.float32(133.9371), np.float32(207.24301)], 'left_eye': [np.float32(138.63853), np.float32(207.32565)], 'nose': [np.float32(134.78163), np.float32(210.50623)], 'mouth_right': [np.float32(134.2779), np.float32(213.5499)], 'mouth_left': [np.float32(138.0626), np.float32(213.47914)]}}, 'face_3': {'score': np.float64(0.9970555901527405), 'facial_area': [np.int64(60), np.int64(199), np.int64(76), np.int64(217)], 'landmarks': {'right_eye': [np.float32(67.177666), np.float32(207.19783)], 'left_eye': [np.float32(73.82506), np.float32(206.95467)], 'nose': [np.float32(71.64368), np.float32(210.53406)], 'mouth_right': [np.float32(68.16657), np.float32(213.86078)], 'mouth_left': [np.float32(73.22417), np.float32(213.7445)]}}, 'face_4': {'score': np.float64(0.9954697489738464), 'facial_area': [np.int64(291), np.int64(149), np.int64(331), np.int64(198)], 'landmarks': {'right_eye': [np.float32(301.70007), np.float32(168.61005)], 'left_eye': [np.float32(319.95612), np.float32(169.1402)], 'nose': [np.float32(309.99072), np.float32(174.12183)], 'mouth_right': [np.float32(302.23407), np.float32(184.2017)], 'mouth_left': [np.float32(317.6302), np.float32(184.53653)]}}}\n",
      "Warning: Face count mismatch for 0424.jpg. Expected 5, found 0.\n",
      "Processing 0650.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9997720718383789), 'facial_area': [np.int64(235), np.int64(136), np.int64(434), np.int64(379)], 'landmarks': {'right_eye': [np.float32(366.29324), np.float32(236.02101)], 'left_eye': [np.float32(417.66116), np.float32(247.5934)], 'nose': [np.float32(424.14188), np.float32(292.13074)], 'mouth_right': [np.float32(351.82858), np.float32(319.89963)], 'mouth_left': [np.float32(394.7497), np.float32(329.41974)]}}}\n",
      "Warning: Face count mismatch for 0650.jpg. Expected 1, found 0.\n",
      "Processing 0477.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9907827973365784), 'facial_area': [np.int64(262), np.int64(214), np.int64(285), np.int64(241)], 'landmarks': {'right_eye': [np.float32(276.88022), np.float32(224.10129)], 'left_eye': [np.float32(281.32632), np.float32(223.84148)], 'nose': [np.float32(283.93057), np.float32(228.02982)], 'mouth_right': [np.float32(278.84415), np.float32(233.82149)], 'mouth_left': [np.float32(282.06155), np.float32(234.00966)]}}, 'face_2': {'score': np.float64(0.9825137257575989), 'facial_area': [np.int64(368), np.int64(209), np.int64(390), np.int64(233)], 'landmarks': {'right_eye': [np.float32(371.7759), np.float32(218.84547)], 'left_eye': [np.float32(381.01907), np.float32(219.04843)], 'nose': [np.float32(374.5215), np.float32(222.96677)], 'mouth_right': [np.float32(372.36853), np.float32(227.43561)], 'mouth_left': [np.float32(379.27725), np.float32(227.56943)]}}, 'face_3': {'score': np.float64(0.9482821822166443), 'facial_area': [np.int64(408), np.int64(221), np.int64(434), np.int64(250)], 'landmarks': {'right_eye': [np.float32(412.34973), np.float32(233.88591)], 'left_eye': [np.float32(413.5326), np.float32(232.34714)], 'nose': [np.float32(413.1305), np.float32(240.09814)], 'mouth_right': [np.float32(420.3441), np.float32(244.93437)], 'mouth_left': [np.float32(420.91357), np.float32(243.7245)]}}}\n",
      "Warning: Face count mismatch for 0477.jpg. Expected 3, found 0.\n",
      "Processing 0071.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9975271821022034), 'facial_area': [np.int64(442), np.int64(114), np.int64(545), np.int64(213)], 'landmarks': {'right_eye': [np.float32(469.89294), np.float32(148.0872)], 'left_eye': [np.float32(516.62885), np.float32(154.56723)], 'nose': [np.float32(487.89984), np.float32(165.28267)], 'mouth_right': [np.float32(466.18527), np.float32(181.9068)], 'mouth_left': [np.float32(506.72638), np.float32(187.25717)]}}}\n",
      "Warning: Face count mismatch for 0071.jpg. Expected 1, found 0.\n",
      "Processing 0261.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9971622824668884), 'facial_area': [np.int64(111), np.int64(89), np.int64(165), np.int64(136)], 'landmarks': {'right_eye': [np.float32(122.26427), np.float32(109.54531)], 'left_eye': [np.float32(146.04062), np.float32(109.5628)], 'nose': [np.float32(131.27522), np.float32(118.84248)], 'mouth_right': [np.float32(124.94098), np.float32(125.375824)], 'mouth_left': [np.float32(142.72707), np.float32(125.23558)]}}, 'face_2': {'score': np.float64(0.9935561418533325), 'facial_area': [np.int64(358), np.int64(106), np.int64(398), np.int64(143)], 'landmarks': {'right_eye': [np.float32(366.17206), np.float32(119.922585)], 'left_eye': [np.float32(384.19455), np.float32(118.5925)], 'nose': [np.float32(374.20584), np.float32(126.76418)], 'mouth_right': [np.float32(368.8011), np.float32(132.9347)], 'mouth_left': [np.float32(384.19467), np.float32(131.90833)]}}, 'face_3': {'score': np.float64(0.9226416945457458), 'facial_area': [np.int64(554), np.int64(96), np.int64(583), np.int64(132)], 'landmarks': {'right_eye': [np.float32(575.6688), np.float32(110.09895)], 'left_eye': [np.float32(578.99695), np.float32(109.51899)], 'nose': [np.float32(582.2844), np.float32(116.67798)], 'mouth_right': [np.float32(575.60144), np.float32(124.02818)], 'mouth_left': [np.float32(577.4279), np.float32(124.018684)]}}}\n",
      "Warning: Face count mismatch for 0261.jpg. Expected 3, found 0.\n",
      "Processing 0810.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.998863160610199), 'facial_area': [np.int64(195), np.int64(140), np.int64(354), np.int64(278)], 'landmarks': {'right_eye': [np.float32(251.7975), np.float32(207.29796)], 'left_eye': [np.float32(323.893), np.float32(200.81061)], 'nose': [np.float32(298.61728), np.float32(243.7616)], 'mouth_right': [np.float32(261.79584), np.float32(255.88284)], 'mouth_left': [np.float32(316.68546), np.float32(250.49551)]}}}\n",
      "Warning: Face count mismatch for 0810.jpg. Expected 1, found 0.\n",
      "Processing 0399.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9995383024215698), 'facial_area': [np.int64(266), np.int64(215), np.int64(313), np.int64(275)], 'landmarks': {'right_eye': [np.float32(271.52875), np.float32(240.8919)], 'left_eye': [np.float32(291.14862), np.float32(238.57596)], 'nose': [np.float32(276.94693), np.float32(248.6975)], 'mouth_right': [np.float32(274.0929), np.float32(260.69266)], 'mouth_left': [np.float32(291.004), np.float32(258.69067)]}}, 'face_2': {'score': np.float64(0.9993613958358765), 'facial_area': [np.int64(114), np.int64(207), np.int64(164), np.int64(272)], 'landmarks': {'right_eye': [np.float32(125.10842), np.float32(232.59651)], 'left_eye': [np.float32(148.28154), np.float32(233.94989)], 'nose': [np.float32(133.69453), np.float32(242.57346)], 'mouth_right': [np.float32(124.878525), np.float32(253.2263)], 'mouth_left': [np.float32(146.35045), np.float32(254.27142)]}}, 'face_3': {'score': np.float64(0.9991983771324158), 'facial_area': [np.int64(459), np.int64(189), np.int64(526), np.int64(275)], 'landmarks': {'right_eye': [np.float32(476.8204), np.float32(226.65373)], 'left_eye': [np.float32(508.6474), np.float32(223.84804)], 'nose': [np.float32(494.17493), np.float32(239.85547)], 'mouth_right': [np.float32(482.99707), np.float32(256.67383)], 'mouth_left': [np.float32(508.58038), np.float32(254.25075)]}}}\n",
      "Warning: Face count mismatch for 0399.jpg. Expected 3, found 0.\n",
      "Processing 0353.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9995479583740234), 'facial_area': [np.int64(206), np.int64(93), np.int64(404), np.int64(289)], 'landmarks': {'right_eye': [np.float32(256.0084), np.float32(175.78072)], 'left_eye': [np.float32(353.1912), np.float32(172.89296)], 'nose': [np.float32(303.8161), np.float32(203.16919)], 'mouth_right': [np.float32(261.98917), np.float32(243.25232)], 'mouth_left': [np.float32(344.42062), np.float32(240.03706)]}}}\n",
      "Warning: Face count mismatch for 0353.jpg. Expected 1, found 0.\n",
      "Processing 0475.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9889435172080994), 'facial_area': [np.int64(303), np.int64(197), np.int64(334), np.int64(225)], 'landmarks': {'right_eye': [np.float32(309.13452), np.float32(210.28384)], 'left_eye': [np.float32(322.5969), np.float32(208.3057)], 'nose': [np.float32(315.55255), np.float32(213.80125)], 'mouth_right': [np.float32(312.54172), np.float32(219.49208)], 'mouth_left': [np.float32(322.79556), np.float32(217.89684)]}}, 'face_2': {'score': np.float64(0.9847673773765564), 'facial_area': [np.int64(216), np.int64(190), np.int64(247), np.int64(221)], 'landmarks': {'right_eye': [np.float32(226.16675), np.float32(203.4306)], 'left_eye': [np.float32(240.01933), np.float32(201.8181)], 'nose': [np.float32(235.0239), np.float32(207.50444)], 'mouth_right': [np.float32(230.45386), np.float32(214.16037)], 'mouth_left': [np.float32(240.3739), np.float32(212.92381)]}}, 'face_3': {'score': np.float64(0.9769963622093201), 'facial_area': [np.int64(420), np.int64(179), np.int64(444), np.int64(208)], 'landmarks': {'right_eye': [np.float32(424.19855), np.float32(188.83807)], 'left_eye': [np.float32(426.59204), np.float32(189.36526)], 'nose': [np.float32(422.7638), np.float32(196.31305)], 'mouth_right': [np.float32(428.91437), np.float32(201.58197)], 'mouth_left': [np.float32(430.30698), np.float32(201.71797)]}}, 'face_4': {'score': np.float64(0.9087047576904297), 'facial_area': [np.int64(573), np.int64(177), np.int64(624), np.int64(225)], 'landmarks': {'right_eye': [np.float32(580.2771), np.float32(198.8726)], 'left_eye': [np.float32(598.8382), np.float32(197.18298)], 'nose': [np.float32(586.9581), np.float32(207.01941)], 'mouth_right': [np.float32(587.8482), np.float32(216.20088)], 'mouth_left': [np.float32(599.7467), np.float32(214.8454)]}}}\n",
      "Warning: Face count mismatch for 0475.jpg. Expected 4, found 0.\n",
      "Processing 0623.jpg...\n",
      "Detections: {'face_1': {'score': np.float64(0.9986798167228699), 'facial_area': [np.int64(154), np.int64(49), np.int64(224), np.int64(113)], 'landmarks': {'right_eye': [np.float32(179.79568), np.float32(73.78108)], 'left_eye': [np.float32(209.7706), np.float32(77.64728)], 'nose': [np.float32(195.73906), np.float32(88.41193)], 'mouth_right': [np.float32(177.19829), np.float32(94.50316)], 'mouth_left': [np.float32(204.22937), np.float32(97.617714)]}}, 'face_2': {'score': np.float64(0.9941192865371704), 'facial_area': [np.int64(526), np.int64(82), np.int64(562), np.int64(115)], 'landmarks': {'right_eye': [np.float32(532.193), np.float32(97.782974)], 'left_eye': [np.float32(547.4057), np.float32(94.66843)], 'nose': [np.float32(538.7167), np.float32(101.569984)], 'mouth_right': [np.float32(536.3573), np.float32(108.08169)], 'mouth_left': [np.float32(548.2197), np.float32(105.53835)]}}, 'face_3': {'score': np.float64(0.9909118413925171), 'facial_area': [np.int64(417), np.int64(86), np.int64(438), np.int64(116)], 'landmarks': {'right_eye': [np.float32(435.15118), np.float32(97.37829)], 'left_eye': [np.float32(434.50168), np.float32(96.831314)], 'nose': [np.float32(439.41696), np.float32(103.32626)], 'mouth_right': [np.float32(434.2931), np.float32(109.33726)], 'mouth_left': [np.float32(433.68695), np.float32(109.42365)]}}}\n",
      "Warning: Face count mismatch for 0623.jpg. Expected 3, found 0.\n",
      "Processing 0338.jpg...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load training images and labels\u001b[39;00m\n\u001b[1;32m     14\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m load_images(data_folder, label_map\u001b[38;5;241m=\u001b[39mlabel_map)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mextract_and_save_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 72\u001b[0m, in \u001b[0;36mextract_and_save_faces\u001b[0;34m(images, labels, output_folder, batch_size, target_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     71\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 72\u001b[0m face_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces_with_retinaface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(face_boxes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_labels):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Face count mismatch for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(face_boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 44\u001b[0m, in \u001b[0;36mdetect_faces_with_retinaface\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetect_faces_with_retinaface\u001b[39m(image):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetections: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetections\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging log\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         face_boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:123\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m    121\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m im_tensor, im_info, im_scale \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39mpreprocess_image(img, allow_upscaling)\n\u001b[0;32m--> 123\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m net_out \u001b[38;5;241m=\u001b[39m [elt\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m net_out]\n\u001b[1;32m    125\u001b[0m sym_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Example usage\n",
    "data_folder = \"../data/images/cleaned_images\"\n",
    "output_folder = \"../data/faces5_train\"\n",
    "label_csv_path = \"../data/labels/clean_data.csv\"\n",
    "batch_size = 50\n",
    "\n",
    "# Load label data\n",
    "label_data = pd.read_csv(label_csv_path)\n",
    "label_data['label_name'] = label_data['label_name'].apply(eval)  # Convert string to list\n",
    "label_map = dict(zip(label_data['image'].astype(str).str.zfill(4) + \".jpg\", label_data['label_name']))\n",
    "\n",
    "# Load training images and labels\n",
    "images, labels = load_images(data_folder, label_map=label_map)\n",
    "\n",
    "\n",
    "extract_and_save_faces(images, labels, output_folder, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
