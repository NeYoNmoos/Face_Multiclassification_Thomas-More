{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from retinaface import RetinaFace\n",
    "import gc\n",
    "\n",
    "# Initialize MTCNN\n",
    "detector = MTCNN()\n",
    "\n",
    "def preprocess_face(face_img, target_size=(224, 224)):\n",
    "    try:\n",
    "        if face_img is None or face_img.size == 0:\n",
    "            return None\n",
    "\n",
    "        # Convert to RGB if needed\n",
    "        if len(face_img.shape) == 2:\n",
    "            face_img = cv2.cvtColor(face_img, cv2.COLOR_GRAY2RGB)\n",
    "        elif face_img.shape[2] == 4:\n",
    "            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGRA2RGB)\n",
    "        elif face_img.shape[2] == 3:\n",
    "            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize while maintaining aspect ratio\n",
    "        aspect_ratio = face_img.shape[1] / face_img.shape[0]\n",
    "        if aspect_ratio > 1:\n",
    "            new_width = target_size[0]\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "        else:\n",
    "            new_height = target_size[1]\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "        resized = cv2.resize(face_img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Create blank canvas\n",
    "        final_img = np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # Center the image\n",
    "        y_offset = (target_size[0] - new_height) // 2\n",
    "        x_offset = (target_size[1] - new_width) // 2\n",
    "        final_img[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized\n",
    "\n",
    "        return final_img\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing face: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def detect_faces(image):\n",
    "    detections = detector.detect_faces(image)\n",
    "    face_boxes = [\n",
    "        detection['box']\n",
    "        for detection in detections if detection['confidence'] > 0.9\n",
    "    ]\n",
    "    return sorted(face_boxes, key=lambda x: x[0])  # Sort left-to-right by x-coordinate\n",
    "\n",
    "def detect_faces_test(image):\n",
    "    detections = detector.detect_faces(image)\n",
    "    face_boxes = [\n",
    "        detection['box']\n",
    "        for detection in detections if detection['confidence'] > 0.9\n",
    "    ]\n",
    "    if not face_boxes:\n",
    "        face_boxes = [\n",
    "        detection['box']\n",
    "        for detection in detections if detection['confidence'] > 0.7]\n",
    "    if not face_boxes:\n",
    "        face_boxes = [\n",
    "        detection['box']\n",
    "        for detection in detections if detection['confidence'] > 0.4]\n",
    "    if not face_boxes:\n",
    "        face_boxes = [\n",
    "        detection['box']\n",
    "        for detection in detections]\n",
    "    if not face_boxes:\n",
    "        print(\"No faces detected. Forcing a default bounding box.\")\n",
    "        height, width = image.shape[:2]\n",
    "        box_size = min(height, width) // 3  # Default box size as one-third of the image size\n",
    "        x = (width - box_size) // 2\n",
    "        y = (height - box_size) // 2\n",
    "        face_boxes = [[x, y, box_size, box_size]]\n",
    "    return sorted(face_boxes, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_faces(images, labels, output_folder, batch_size=50, target_size=(224, 224)):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for batch_start in range(0, len(images), batch_size):\n",
    "        batch_images = images[batch_start:batch_start + batch_size]\n",
    "        batch_labels = labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "        for (filename, image), image_labels in zip(batch_images, batch_labels):\n",
    "            print(f\"Processing {filename}...\")\n",
    "\n",
    "            if image_labels == [\"nothing\"]:\n",
    "                continue\n",
    "\n",
    "            # Convert to RGB for detection\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            face_boxes = detect_faces(rgb_image)\n",
    "\n",
    "            # Skip if face count does not match label count\n",
    "            # if len(face_boxes) != len(image_labels):\n",
    "            #     print(f\"Skipping {filename} - Face count mismatch ({len(face_boxes)} faces, {len(image_labels)} labels)\")\n",
    "            #     continue\n",
    "\n",
    "            # Process and save each face\n",
    "            for i, (box, label) in enumerate(zip(face_boxes, image_labels)):\n",
    "                x, y, w, h = box\n",
    "\n",
    "                # Add margin to bounding box\n",
    "                margin = int(max(w, h) * 0.2)\n",
    "                x = max(0, x - margin)\n",
    "                y = max(0, y - margin)\n",
    "                w = min(w + 2 * margin, image.shape[1] - x)\n",
    "                h = min(h + 2 * margin, image.shape[0] - y)\n",
    "\n",
    "                # Extract face region\n",
    "                face = image[y:y+h, x:x+w]\n",
    "\n",
    "                # if face.size == 0 or face.shape[0] < 50 or face.shape[1] < 50:\n",
    "                #     print(f\"Warning: Face too small in {filename}\")\n",
    "                #     continue\n",
    "\n",
    "                # Preprocess face\n",
    "                processed_face = preprocess_face(face, target_size)\n",
    "                if processed_face is None:\n",
    "                    print(f\"Warning: Could not process face in {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Save face image in label folder\n",
    "                label_folder = os.path.join(output_folder, label.lower())\n",
    "                os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "                face_filename = f\"{os.path.splitext(filename)[0]}_face_{i}.jpg\"\n",
    "                face_path = os.path.join(label_folder, face_filename)\n",
    "                cv2.imwrite(face_path, cv2.cvtColor(processed_face, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_folder, label_map=None):\n",
    "    images = []\n",
    "    image_labels = []\n",
    "\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            images.append((filename, img))\n",
    "            if label_map:\n",
    "                image_labels.append(label_map.get(filename, []))  # Default to empty list for test images\n",
    "\n",
    "    return images, image_labels if label_map else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect images from Trainingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training images...\n",
      "Processing 0032.jpg...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Crop and save faces\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mextract_and_save_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCropped faces have been saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mextract_and_save_faces\u001b[0;34m(images, labels, output_folder, batch_size, target_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Convert to RGB for detection\u001b[39;00m\n\u001b[1;32m     15\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 16\u001b[0m face_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Skip if face count does not match label count\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# if len(face_boxes) != len(image_labels):\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     print(f\"Skipping {filename} - Face count mismatch ({len(face_boxes)} faces, {len(image_labels)} labels)\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Process and save each face\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (box, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(face_boxes, image_labels)):\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetect_faces\u001b[39m(image):\n\u001b[0;32m---> 51\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     face_boxes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     53\u001b[0m         detection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m detections \u001b[38;5;28;01mif\u001b[39;00m detection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     55\u001b[0m     ]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(face_boxes, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:96\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m    100\u001b[0m nms_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:54\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m model  \u001b[38;5;66;03m# singleton design pattern\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[1;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m---> 54\u001b[0m         \u001b[43mretinaface_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     55\u001b[0m         input_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),),\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/model/retinaface_model.py:1027\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1019\u001b[0m ssh_m3_det_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[1;32m   1020\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m )(ssh_m3_det_conv1)\n\u001b[1;32m   1023\u001b[0m ssh_m3_det_context_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[1;32m   1024\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_context_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m )(ssh_m3_det_context_conv1)\n\u001b[0;32m-> 1027\u001b[0m x1_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssh_c3_up\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m x2_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(ssh_c2_lateral_relu)\n\u001b[1;32m   1029\u001b[0m offsets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, (x1_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, (x1_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "train_image_folder = \"../data/olda_data/cleaned_images\"\n",
    "label_csv_path = \"../data/labels/clean_data.csv\"\n",
    "output_folder = \"../data/faces/train5_faces\"\n",
    "\n",
    "# Load label data\n",
    "import pandas as pd\n",
    "label_data = pd.read_csv(label_csv_path)\n",
    "label_data['label_name'] = label_data['label_name'].apply(eval)  # Convert string to list\n",
    "label_map = dict(zip(label_data['image'].astype(str).str.zfill(4) + \".jpg\", label_data['label_name']))\n",
    "\n",
    "# Load training images and labels\n",
    "train_images, train_labels = load_images(train_image_folder, label_map=label_map)\n",
    "\n",
    "# Crop and save faces\n",
    "print(\"Processing training images...\")\n",
    "extract_and_save_faces(train_images, labels=train_labels, output_folder=output_folder, batch_size=50)\n",
    "print(\"Cropped faces have been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_test_faces(images, output_folder, batch_size=50, target_size=(224, 224)):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for batch_start in range(0, len(images), batch_size):\n",
    "        batch_images = images[batch_start:batch_start + batch_size]\n",
    "\n",
    "        for filename, image in batch_images:\n",
    "            print(f\"Processing {filename}...\")\n",
    "\n",
    "            # Convert to RGB for detection'\n",
    "            # rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            face_boxes = detect_faces_test(image)\n",
    "\n",
    "            # Skip images with no detected faces\n",
    "            if not face_boxes:\n",
    "                print(f\"No faces detected in {filename}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Process and save each face\n",
    "            for i, box in enumerate(face_boxes):\n",
    "                x, y, w, h = box\n",
    "\n",
    "                # Add margin to bounding box\n",
    "                margin = int(max(w, h) * 0.2)\n",
    "                x = max(0, x - margin)\n",
    "                y = max(0, y - margin)\n",
    "                w = min(w + 2 * margin, image.shape[1] - x)\n",
    "                h = min(h + 2 * margin, image.shape[0] - y)\n",
    "\n",
    "                # Extract face region\n",
    "                face = image[y:y+h, x:x+w]\n",
    "\n",
    "                # if face.size == 0 or face.shape[0] < 50 or face.shape[1] < 50:\n",
    "                #     print(f\"Warning: Face too small in {filename}\")\n",
    "                #     continue\n",
    "\n",
    "                # Preprocess face\n",
    "                processed_face = preprocess_face(face, target_size)\n",
    "                if processed_face is None:\n",
    "                    print(f\"Warning: Could not process face in {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Save face image in a common folder for test faces\n",
    "                face_filename = f\"{os.path.splitext(filename)[0]}_face_{i}.jpg\"\n",
    "                face_path = os.path.join(output_folder, face_filename)\n",
    "                cv2.imwrite(face_path, cv2.cvtColor(processed_face, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test images...\n",
      "Processing 0427.jpg...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Crop and save test faces\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing test images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mextract_and_save_test_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCropped faces from test set have been saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mextract_and_save_test_faces\u001b[0;34m(images, output_folder, batch_size, target_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert to RGB for detection'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m face_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Skip images with no detected faces\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m face_boxes:\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mdetect_faces_test\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetect_faces_test\u001b[39m(image):\n\u001b[0;32m---> 59\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     face_boxes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     61\u001b[0m         detection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m detections \u001b[38;5;28;01mif\u001b[39;00m detection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     63\u001b[0m     ]\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# if not face_boxes:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m#     face_boxes = [\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#     detection['box']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m#     detection['box']\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#     for detection in detections if detection['confidence'] > 0.4]\u001b[39;00m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:96\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m    100\u001b[0m nms_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:54\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m model  \u001b[38;5;66;03m# singleton design pattern\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[1;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m---> 54\u001b[0m         \u001b[43mretinaface_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     55\u001b[0m         input_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),),\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/model/retinaface_model.py:1027\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1019\u001b[0m ssh_m3_det_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[1;32m   1020\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m )(ssh_m3_det_conv1)\n\u001b[1;32m   1023\u001b[0m ssh_m3_det_context_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[1;32m   1024\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_context_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m )(ssh_m3_det_context_conv1)\n\u001b[0;32m-> 1027\u001b[0m x1_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssh_c3_up\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m x2_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(ssh_c2_lateral_relu)\n\u001b[1;32m   1029\u001b[0m offsets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, (x1_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, (x1_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "test_image_folder = \"../data/images/test_images/cleaned_images\"\n",
    "test_output_folder = \"../data/faces/test6_faces\"\n",
    "\n",
    "# Load test images\n",
    "test_images, _ = load_images(test_image_folder)\n",
    "\n",
    "# Crop and save test faces\n",
    "print(\"Processing test images...\")\n",
    "extract_and_save_test_faces(test_images, output_folder=test_output_folder, batch_size=50)\n",
    "print(\"Cropped faces from test set have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
