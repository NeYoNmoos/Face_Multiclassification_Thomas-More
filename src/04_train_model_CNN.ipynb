{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3666 images for training.\n"
     ]
    }
   ],
   "source": [
    "### Cell 1: Import Libraries and Load Training Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Function to load and preprocess training data\n",
    "def load_training_data(data_folder, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    current_label = 0\n",
    "    data_gen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    for person_name in os.listdir(data_folder):\n",
    "        person_folder = os.path.join(data_folder, person_name)\n",
    "        if os.path.isdir(person_folder):\n",
    "            label_map[current_label] = person_name\n",
    "            for filename in os.listdir(person_folder):\n",
    "                img_path = os.path.join(person_folder, filename)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img_resized = cv2.resize(img, target_size)\n",
    "                    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                    augmented_imgs = [img_rgb] + [\n",
    "                        data_gen.random_transform(img_rgb) for _ in range(5)\n",
    "                    ]\n",
    "                    images.extend(augmented_imgs)\n",
    "                    labels.extend([current_label] * len(augmented_imgs))\n",
    "            current_label += 1\n",
    "\n",
    "    return np.array(images), np.array(labels), label_map\n",
    "\n",
    "# Load training data\n",
    "data_folder = \"../data/faces3\"\n",
    "images, labels, label_map = load_training_data(data_folder)\n",
    "\n",
    "# Flatten images for PCA\n",
    "def extract_eigenfaces(images, num_components=50):\n",
    "    flattened_images = images.reshape(len(images), -1)\n",
    "    pca = PCA(n_components=num_components, whiten=True, random_state=42)\n",
    "    eigenfaces = pca.fit_transform(flattened_images)\n",
    "    return eigenfaces, pca\n",
    "\n",
    "print(f\"Loaded {len(images)} images for training.\")\n",
    "eigenfaces, pca = extract_eigenfaces(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the enhanced CNN model with PCA...\n",
      "Epoch 1/10\n",
      "92/92 [==============================] - 47s 476ms/step - loss: 2.2173 - accuracy: 0.2599 - val_loss: 3.7473 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 42s 455ms/step - loss: 1.2204 - accuracy: 0.6248 - val_loss: 5.0664 - val_accuracy: 0.0014\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 41s 451ms/step - loss: 0.5489 - accuracy: 0.8431 - val_loss: 6.5665 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 43s 465ms/step - loss: 0.2265 - accuracy: 0.9461 - val_loss: 6.2652 - val_accuracy: 0.0054\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 42s 455ms/step - loss: 0.0906 - accuracy: 0.9823 - val_loss: 7.0339 - val_accuracy: 0.0082\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 43s 471ms/step - loss: 0.0471 - accuracy: 0.9908 - val_loss: 7.7132 - val_accuracy: 0.0068\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 45s 485ms/step - loss: 0.0237 - accuracy: 0.9980 - val_loss: 8.2357 - val_accuracy: 0.0232\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 46s 498ms/step - loss: 0.0134 - accuracy: 0.9983 - val_loss: 9.1122 - val_accuracy: 0.0191\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 49s 531ms/step - loss: 0.0128 - accuracy: 0.9993 - val_loss: 10.3297 - val_accuracy: 0.0150\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 44s 475ms/step - loss: 0.0088 - accuracy: 0.9993 - val_loss: 10.5651 - val_accuracy: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyon/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../data/cnn_pca_model.h5\n"
     ]
    }
   ],
   "source": [
    "### Cell 2: Create and Train the Enhanced Model\n",
    "def create_cnn_with_pca_model(num_classes, input_shape=(224, 224, 3), num_components=50):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_components, activation='linear', name=\"pca_layer\")(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "num_classes = len(label_map)\n",
    "model = create_cnn_with_pca_model(num_classes)\n",
    "\n",
    "print(\"Training the enhanced CNN model with PCA...\")\n",
    "history = model.fit(images, labels, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"../data/cnn_pca_model.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 447ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "### Cell 3: Load Model and Predict on Test Data\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def predict_faces_on_folder_with_pca(model_path, test_faces_folder, label_map, target_size=(224, 224)):\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(test_faces_folder):\n",
    "        test_image_path = os.path.join(test_faces_folder, filename)\n",
    "\n",
    "        test_img = cv2.imread(test_image_path)\n",
    "        if test_img is None:\n",
    "            print(f\"Failed to read {test_image_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        test_img_resized = cv2.resize(test_img, target_size)\n",
    "        test_img_preprocessed = np.expand_dims(test_img_resized, axis=0) / 255.0\n",
    "\n",
    "        predictions = model.predict(test_img_preprocessed)\n",
    "        label_idx = np.argmax(predictions)\n",
    "        confidence = predictions[0][label_idx]\n",
    "        person_name = label_map[label_idx]\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"predicted_label\": person_name,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Predict on test data\n",
    "model_path = \"../data/cnn_pca_model.h5\"\n",
    "test_faces_folder = \"../data/faces3_test\"\n",
    "results = predict_faces_on_folder_with_pca(model_path, test_faces_folder, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 4: Group Results by Image\n",
    "def group_results_by_image(results, confidence_threshold=0):\n",
    "    grouped_data = {}\n",
    "    for result in results:\n",
    "        base_filename = result[\"filename\"].split(\"_face\")[0]\n",
    "        if result[\"confidence\"] < confidence_threshold:\n",
    "            continue\n",
    "        if base_filename not in grouped_data:\n",
    "            grouped_data[base_filename] = []\n",
    "        grouped_data[base_filename].append(result[\"predicted_label\"].lower())\n",
    "\n",
    "    grouped_results = [{\"filename\": filename, \"predicted_labels\": labels} for filename, labels in grouped_data.items()]\n",
    "    return grouped_results\n",
    "\n",
    "grouped_results = group_results_by_image(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to ../submission.csv\n"
     ]
    }
   ],
   "source": [
    "### Cell 5: Create Submission File\n",
    "def create_submission_csv_from_grouped_results(grouped_results, output_csv_path, test_images_folder):\n",
    "    all_filenames = sorted(\n",
    "        [os.path.splitext(filename)[0] for filename in os.listdir(test_images_folder) \n",
    "         if filename.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    )\n",
    "\n",
    "    grouped_dict = {group[\"filename\"].split(\".\")[0]: \";\".join(group[\"predicted_labels\"]) for group in grouped_results}\n",
    "\n",
    "    submission_data = []\n",
    "    for filename in all_filenames:\n",
    "        label_name = grouped_dict.get(filename, \"nothing\")\n",
    "        submission_data.append({\"image\": filename, \"label_name\": label_name})\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Submission file saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "test_images_folder = \"../data/images/test_images/cleaned_images\"\n",
    "output_csv_path = \"../submission.csv\"\n",
    "create_submission_csv_from_grouped_results(grouped_results, output_csv_path, test_images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
