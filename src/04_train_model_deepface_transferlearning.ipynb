{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 611 images across 14 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras_facenet import FaceNet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Function to preprocess images for TensorFlow\n",
    "def preprocess_image(image_path, target_size=(160, 160)):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array /= 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Load dataset and preprocess\n",
    "def load_data(data_folder, target_size=(160, 160)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    current_label = 0\n",
    "\n",
    "    for person_name in os.listdir(data_folder):\n",
    "        person_folder = os.path.join(data_folder, person_name)\n",
    "        if os.path.isdir(person_folder):\n",
    "            label_map[current_label] = person_name\n",
    "            for filename in os.listdir(person_folder):\n",
    "                img_path = os.path.join(person_folder, filename)\n",
    "                img = preprocess_image(img_path, target_size)\n",
    "                images.append(img)\n",
    "                labels.append(current_label)\n",
    "            current_label += 1\n",
    "\n",
    "    return np.array(images), np.array(labels), label_map\n",
    "\n",
    "data_folder = \"../data/faces3\"\n",
    "print(\"Loading dataset...\")\n",
    "images, labels, label_map = load_data(data_folder)\n",
    "print(f\"Loaded {len(images)} images across {len(label_map)} classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained FaceNet model\n",
    "facenet = FaceNet()\n",
    "base_model = facenet.model\n",
    "# base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Freeze all layers in the base FaceNet model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(len(label_map), activation='softmax')(x)\n",
    "\n",
    "# Define the new model\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyon/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 438ms/step - accuracy: 0.2907 - loss: 2.5644 - val_accuracy: 0.7724 - val_loss: 2.2885\n",
      "Epoch 2/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 313ms/step - accuracy: 0.8405 - loss: 2.1149 - val_accuracy: 0.7967 - val_loss: 1.8376\n",
      "Epoch 3/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 320ms/step - accuracy: 0.8806 - loss: 1.5874 - val_accuracy: 0.8211 - val_loss: 1.4002\n",
      "Epoch 4/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 327ms/step - accuracy: 0.8964 - loss: 1.1237 - val_accuracy: 0.8374 - val_loss: 1.0497\n",
      "Epoch 5/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 317ms/step - accuracy: 0.9117 - loss: 0.8168 - val_accuracy: 0.8780 - val_loss: 0.8068\n",
      "Epoch 6/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 317ms/step - accuracy: 0.9590 - loss: 0.5837 - val_accuracy: 0.8943 - val_loss: 0.6543\n",
      "Epoch 7/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 310ms/step - accuracy: 0.9704 - loss: 0.4292 - val_accuracy: 0.9024 - val_loss: 0.5561\n",
      "Epoch 8/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 317ms/step - accuracy: 0.9578 - loss: 0.3944 - val_accuracy: 0.9024 - val_loss: 0.4990\n",
      "Epoch 9/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 310ms/step - accuracy: 0.9548 - loss: 0.3376 - val_accuracy: 0.9024 - val_loss: 0.4634\n",
      "Epoch 10/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 314ms/step - accuracy: 0.9624 - loss: 0.2976 - val_accuracy: 0.9024 - val_loss: 0.4423\n",
      "Epoch 11/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 310ms/step - accuracy: 0.9455 - loss: 0.2995 - val_accuracy: 0.9106 - val_loss: 0.4234\n",
      "Epoch 12/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 315ms/step - accuracy: 0.9441 - loss: 0.2663 - val_accuracy: 0.9187 - val_loss: 0.4053\n",
      "Epoch 13/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.9663 - loss: 0.2225 - val_accuracy: 0.9187 - val_loss: 0.3955\n",
      "Epoch 14/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.9754 - loss: 0.1985 - val_accuracy: 0.9187 - val_loss: 0.3820\n",
      "Epoch 15/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.9701 - loss: 0.1873 - val_accuracy: 0.9268 - val_loss: 0.3801\n",
      "Epoch 16/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.9802 - loss: 0.1740 - val_accuracy: 0.9268 - val_loss: 0.3769\n",
      "Epoch 17/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - accuracy: 0.9642 - loss: 0.1902 - val_accuracy: 0.9268 - val_loss: 0.3764\n",
      "Epoch 18/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.9803 - loss: 0.1499 - val_accuracy: 0.9268 - val_loss: 0.3769\n",
      "Epoch 19/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 319ms/step - accuracy: 0.9801 - loss: 0.1457 - val_accuracy: 0.9187 - val_loss: 0.3798\n",
      "Epoch 20/20\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 305ms/step - accuracy: 0.9660 - loss: 0.1564 - val_accuracy: 0.9187 - val_loss: 0.3809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data generators\n",
    "train_datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Fit data generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"../models/facenet_transfer_learning.keras\")\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 804ms/step\n",
      "Predicted Names: ['senne', 'senne', 'youssef', 'konrad', 'seppe', 'alper', 'senne', 'seppe', 'raul', 'raul', 'seppe', 'bart', 'lasse', 'seppe', 'florian', 'alper', 'daiane', 'raul', 'youssef', 'youssef', 'florian', 'akif', 'konrad', 'bart', 'akif', 'alper', 'seppe', 'konrad', 'senne', 'michiel', 'matthias', 'nelli', 'senne', 'youssef', 'matthias', 'seppe', 'bart', 'alper', 'akif', 'raul', 'youssef', 'seppe', 'senne', 'senne', 'akif', 'seppe', 'michiel', 'raul', 'youssef', 'seppe', 'nelli', 'seppe', 'matthias', 'daiane', 'alper', 'raul', 'youssef', 'youssef', 'seppe', 'alper', 'alper', 'lasse', 'michiel', 'lasse', 'lasse', 'raul', 'florian', 'matthias', 'bart', 'senne', 'lasse', 'senne', 'youssef', 'seppe', 'konrad', 'lasse', 'michiel', 'seppe', 'nelli', 'florian', 'senne', 'nelli', 'seppe', 'michiel', 'raul', 'youssef', 'lasse', 'youssef', 'akif', 'florian', 'matthias', 'matthias', 'matthias', 'florian', 'youssef', 'matthias', 'youssef', 'alper', 'matthias', 'youssef', 'raul', 'bart', 'florian', 'nelli', 'seppe', 'daiane', 'daiane', 'youssef', 'youssef', 'lasse', 'lasse', 'daiane', 'seppe', 'daiane', 'konrad', 'akif', 'akif', 'florian', 'seppe', 'akif', 'daiane', 'matthias', 'youssef', 'raul', 'lasse']\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess test data\n",
    "def load_test_data(test_folder, target_size=(160, 160)):\n",
    "    test_images = []\n",
    "    filenames = []\n",
    "    for filename in sorted(os.listdir(test_folder)):\n",
    "        img_path = os.path.join(test_folder, filename)\n",
    "        img = preprocess_image(img_path, target_size)\n",
    "        test_images.append(img)\n",
    "        filenames.append(filename)\n",
    "    return np.array(test_images), filenames\n",
    "\n",
    "test_folder = \"../data/faces4_test\"\n",
    "test_images, test_filenames = load_test_data(test_folder)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Reverse the label map (keys become values and vice versa)\n",
    "label_map_reverse = {int(k): v for k, v in label_map.items()}  # Ensure keys are integers\n",
    "predicted_names = [label_map_reverse[int(label)] for label in predicted_labels]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Names: {predicted_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to ../submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create submission file with grouped predictions and include missing images\n",
    "def create_submission(filenames, predictions, output_path, image_folder):\n",
    "    # Extract base filenames (e.g., \"0037\" from \"0037_face_0\")\n",
    "    base_filenames = [filename.split('_face')[0] for filename in filenames]\n",
    "\n",
    "    # Group predictions by base filename\n",
    "    grouped_results = {}\n",
    "    for base, prediction in zip(base_filenames, predictions):\n",
    "        if base not in grouped_results:\n",
    "            grouped_results[base] = []\n",
    "        grouped_results[base].append(prediction)\n",
    "\n",
    "    # Ensure all filenames in the image folder are included\n",
    "    all_filenames = sorted(\n",
    "        [os.path.splitext(filename)[0] for filename in os.listdir(image_folder) if filename.endswith('.jpg')]\n",
    "    )\n",
    "\n",
    "    # Prepare submission data\n",
    "    submission_data = []\n",
    "    for filename in all_filenames:\n",
    "        if filename in grouped_results:\n",
    "            label_name = \";\".join(grouped_results[filename])\n",
    "        else:\n",
    "            label_name = \"nothing\"  # Add \"nothing\" for missing images\n",
    "        submission_data.append({\"image\": filename, \"label_name\": label_name})\n",
    "\n",
    "    # Save to CSV\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"Submission saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "image_folder = \"../data/images/test_images/cleaned_images\"\n",
    "output_path = \"../submission.csv\"\n",
    "create_submission(test_filenames, predicted_names, output_path, image_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
