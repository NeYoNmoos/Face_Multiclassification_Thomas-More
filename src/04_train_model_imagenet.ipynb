{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 22:24:43.585268: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_initialize_variables' from 'keras.src.backend' (/home/neyon/anaconda3/lib/python3.9/site-packages/keras/src/backend/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EfficientNetB0\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, GlobalAveragePooling2D, Dropout\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/__internal__/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/__internal__/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_initialize_variables' from 'keras.src.backend' (/home/neyon/anaconda3/lib/python3.9/site-packages/keras/src/backend/__init__.py)"
     ]
    }
   ],
   "source": [
    "### Cell 1: Import Libraries and Load Training Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Function to load and preprocess training data\n",
    "def load_training_data(data_folder, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    current_label = 0\n",
    "    data_gen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    for person_name in os.listdir(data_folder):\n",
    "        person_folder = os.path.join(data_folder, person_name)\n",
    "        if os.path.isdir(person_folder):\n",
    "            label_map[current_label] = person_name\n",
    "            for filename in os.listdir(person_folder):\n",
    "                img_path = os.path.join(person_folder, filename)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img_resized = cv2.resize(img, target_size)\n",
    "                    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                    augmented_imgs = [img_rgb] + [\n",
    "                        data_gen.random_transform(img_rgb) for _ in range(5)\n",
    "                    ]\n",
    "                    images.extend(augmented_imgs)\n",
    "                    labels.extend([current_label] * len(augmented_imgs))\n",
    "            current_label += 1\n",
    "\n",
    "    return np.array(images), np.array(labels), label_map\n",
    "\n",
    "# Load training data\n",
    "data_folder = \"../data/faces3\"\n",
    "images, labels, label_map = load_training_data(data_folder)\n",
    "\n",
    "print(f\"Loaded {len(images)} images for training.\")\n",
    "\n",
    "### Cell 2: Create and Train the Enhanced Model\n",
    "def create_efficientnet_model(num_classes, input_shape=(224, 224, 3)):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "num_classes = len(label_map)\n",
    "model = create_efficientnet_model(num_classes)\n",
    "\n",
    "print(\"Training the EfficientNet model...\")\n",
    "history = model.fit(images, labels, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"../data/efficientnet_model.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 941ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "### Cell 3: Load Model and Predict on Test Data\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def predict_faces_on_folder(model_path, test_faces_folder, label_map, target_size=(224, 224)):\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(test_faces_folder):\n",
    "        test_image_path = os.path.join(test_faces_folder, filename)\n",
    "\n",
    "        test_img = cv2.imread(test_image_path)\n",
    "        if test_img is None:\n",
    "            print(f\"Failed to read {test_image_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        test_img_resized = cv2.resize(test_img, target_size)\n",
    "        test_img_preprocessed = np.expand_dims(test_img_resized, axis=0) / 255.0\n",
    "\n",
    "        predictions = model.predict(test_img_preprocessed)\n",
    "        label_idx = np.argmax(predictions)\n",
    "        confidence = predictions[0][label_idx]\n",
    "        person_name = label_map[label_idx]\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"predicted_label\": person_name,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Predict on test data\n",
    "model_path = \"../data/efficientnet_model.h5\"\n",
    "test_faces_folder = \"../data/faces4_test\"\n",
    "results = predict_faces_on_folder(model_path, test_faces_folder, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filename': '0039_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21396542}, {'filename': '0381_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21287395}, {'filename': '0779_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21369146}, {'filename': '0174_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21420985}, {'filename': '0045_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21395609}, {'filename': '0676_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21331804}, {'filename': '0794_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21370153}, {'filename': '0615_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21378538}, {'filename': '0676_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21490164}, {'filename': '0472_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21427628}, {'filename': '0770_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21452568}, {'filename': '0245_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21441802}, {'filename': '0262_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21375112}, {'filename': '0603_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21391776}, {'filename': '0668_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21393575}, {'filename': '0344_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21314996}, {'filename': '0262_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2138769}, {'filename': '0646_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21471949}, {'filename': '0790_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21328178}, {'filename': '0125_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21476015}, {'filename': '0443_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21323577}, {'filename': '0822_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21567763}, {'filename': '0679_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21467222}, {'filename': '0664_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21365894}, {'filename': '0039_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21450828}, {'filename': '0221_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21430565}, {'filename': '0398_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2141705}, {'filename': '0822_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21443802}, {'filename': '0168_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21417914}, {'filename': '0754_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21470472}, {'filename': '0039_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2142655}, {'filename': '0648_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21403721}, {'filename': '0275_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21318088}, {'filename': '0589_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2136872}, {'filename': '0103_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21437077}, {'filename': '0381_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2134985}, {'filename': '0779_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2143628}, {'filename': '0698_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21382293}, {'filename': '0398_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21386845}, {'filename': '0040_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2137223}, {'filename': '0624_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21316428}, {'filename': '0226_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21435687}, {'filename': '0040_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2144514}, {'filename': '0605_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21368612}, {'filename': '0316_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21370852}, {'filename': '0612_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21447305}, {'filename': '0592_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21487832}, {'filename': '0377_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21405137}, {'filename': '0066_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21409628}, {'filename': '0772_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21377136}, {'filename': '0818_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2143995}, {'filename': '0513_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21487407}, {'filename': '0262_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21465361}, {'filename': '0452_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21451172}, {'filename': '0418_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2142817}, {'filename': '0450_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21405613}, {'filename': '0808_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21384735}, {'filename': '0792_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21402465}, {'filename': '0448_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21424192}, {'filename': '0166_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2142931}, {'filename': '0381_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21469754}, {'filename': '0614_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21366695}, {'filename': '0058_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21391408}, {'filename': '0194_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21444447}, {'filename': '0786_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21568649}, {'filename': '0454_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21439567}, {'filename': '0420_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21448185}, {'filename': '0398_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21398196}, {'filename': '0766_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21411735}, {'filename': '0255_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21507281}, {'filename': '0160_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21414909}, {'filename': '0268_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21372269}, {'filename': '0764_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21433859}, {'filename': '0066_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21460153}, {'filename': '0062_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2141555}, {'filename': '0161_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21447805}, {'filename': '0669_face_3.jpg', 'predicted_label': 'daiane', 'confidence': 0.21425413}, {'filename': '0675_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21409239}, {'filename': '0590_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21413971}, {'filename': '0757_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21494384}, {'filename': '0799_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21481188}, {'filename': '0418_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21421133}, {'filename': '0037_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21449208}, {'filename': '0765_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21390902}, {'filename': '0805_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21356878}, {'filename': '0040_face_3.jpg', 'predicted_label': 'daiane', 'confidence': 0.21428616}, {'filename': '0282_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2140463}, {'filename': '0179_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21386942}, {'filename': '0435_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21422985}, {'filename': '0521_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2126187}, {'filename': '0043_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21424524}, {'filename': '0596_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21442612}, {'filename': '0078_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21359478}, {'filename': '0638_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2141739}, {'filename': '0757_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21454938}, {'filename': '0763_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21477467}, {'filename': '0521_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21386036}, {'filename': '0427_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2141377}, {'filename': '0051_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21303116}, {'filename': '0377_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21554413}, {'filename': '0612_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21445723}, {'filename': '0456_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21382287}, {'filename': '0454_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21466164}, {'filename': '0669_face_2.jpg', 'predicted_label': 'daiane', 'confidence': 0.21518339}, {'filename': '0594_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.214132}, {'filename': '0481_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21392593}, {'filename': '0664_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21359648}, {'filename': '0233_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2153372}, {'filename': '0282_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2139069}, {'filename': '0605_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21357368}, {'filename': '0423_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.2139683}, {'filename': '0472_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.2141401}, {'filename': '0599_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21356358}, {'filename': '0139_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.214182}, {'filename': '0444_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21459584}, {'filename': '0194_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21478872}, {'filename': '0641_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21425472}, {'filename': '0416_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21407075}, {'filename': '0825_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21328643}, {'filename': '0786_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21397594}, {'filename': '0521_face_3.jpg', 'predicted_label': 'daiane', 'confidence': 0.21374406}, {'filename': '0043_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21472636}, {'filename': '0584_face_1.jpg', 'predicted_label': 'daiane', 'confidence': 0.21427839}, {'filename': '0452_face_3.jpg', 'predicted_label': 'daiane', 'confidence': 0.21479733}, {'filename': '0420_face_0.jpg', 'predicted_label': 'daiane', 'confidence': 0.21470168}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to ../submission.csv\n"
     ]
    }
   ],
   "source": [
    "### Cell 4: Group Results by Image\n",
    "def group_results_by_image(results, confidence_threshold=0.01):\n",
    "    grouped_data = {}\n",
    "    for result in results:\n",
    "        base_filename = result[\"filename\"].split(\"_face\")[0]\n",
    "        if result[\"confidence\"] < confidence_threshold:\n",
    "            continue\n",
    "        if base_filename not in grouped_data:\n",
    "            grouped_data[base_filename] = []\n",
    "        grouped_data[base_filename].append(result[\"predicted_label\"].lower())\n",
    "\n",
    "    grouped_results = [{\"filename\": filename, \"predicted_labels\": labels} for filename, labels in grouped_data.items()]\n",
    "    return grouped_results\n",
    "\n",
    "grouped_results = group_results_by_image(results)\n",
    "\n",
    "### Cell 5: Create Submission File\n",
    "def create_submission_csv_from_grouped_results(grouped_results, output_csv_path, test_images_folder):\n",
    "    all_filenames = sorted(\n",
    "        [os.path.splitext(filename)[0] for filename in os.listdir(test_images_folder) \n",
    "         if filename.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    )\n",
    "\n",
    "    grouped_dict = {group[\"filename\"].split(\".\")[0]: \";\".join(group[\"predicted_labels\"]) for group in grouped_results}\n",
    "\n",
    "    submission_data = []\n",
    "    for filename in all_filenames:\n",
    "        label_name = grouped_dict.get(filename, \"nothing\")\n",
    "        submission_data.append({\"image\": filename, \"label_name\": label_name})\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Submission file saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "test_images_folder = \"../data/images/test_images/cleaned_images\"\n",
    "output_csv_path = \"../submission.csv\"\n",
    "create_submission_csv_from_grouped_results(grouped_results, output_csv_path, test_images_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
