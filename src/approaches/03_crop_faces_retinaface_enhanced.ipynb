{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import gc\n",
    "\n",
    "# Initialize RetinaFace\n",
    "detector = RetinaFace\n",
    "\n",
    "def assess_face_quality(face_img, min_size=30, min_confidence=0.95):\n",
    "    \"\"\"Assess face quality based on size and detection confidence\"\"\"\n",
    "    if face_img is None or face_img.size == 0:\n",
    "        return False\n",
    "    \n",
    "    height, width = face_img.shape[:2]\n",
    "    if height < min_size or width < min_size:\n",
    "        return False\n",
    "    \n",
    "    # Add basic blur detection\n",
    "    gray = cv2.cvtColor(face_img, cv2.COLOR_RGB2GRAY)\n",
    "    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    if blur_score < 100:  # Adjust threshold as needed\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def detect_faces_enhanced(image):\n",
    "    \"\"\"Enhanced face detection using multiple detection passes\"\"\"\n",
    "    # First pass with RetinaFace\n",
    "    detections = RetinaFace.detect_faces(image)\n",
    "    face_boxes = []\n",
    "    face_confidences = []\n",
    "    \n",
    "    if isinstance(detections, dict):\n",
    "        for _, face_data in detections.items():\n",
    "            facial_area = face_data['facial_area']\n",
    "            confidence = face_data.get('score', 0.0)\n",
    "            landmarks = face_data.get('landmarks', None)\n",
    "            \n",
    "            # Calculate face angle using landmarks if available\n",
    "            if landmarks:\n",
    "                left_eye = landmarks['left_eye']\n",
    "                right_eye = landmarks['right_eye']\n",
    "                angle = np.degrees(np.arctan2(\n",
    "                    right_eye[1] - left_eye[1],\n",
    "                    right_eye[0] - left_eye[0]\n",
    "                ))\n",
    "                \n",
    "                # Skip faces with extreme angles\n",
    "                if abs(angle) > 30:\n",
    "                    continue\n",
    "            \n",
    "            x1, y1, x2, y2 = facial_area\n",
    "            face_boxes.append([x1, y1, x2 - x1, y2 - y1])\n",
    "            face_confidences.append(confidence)\n",
    "    \n",
    "    # Sort faces by confidence\n",
    "    if face_boxes:\n",
    "        sorted_faces = sorted(zip(face_boxes, face_confidences), \n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "        return [box for box, _ in sorted_faces]\n",
    "    \n",
    "    return []\n",
    "\n",
    "def preprocess_face_enhanced(face_img, target_size=(224, 224)):\n",
    "    try:\n",
    "        if not assess_face_quality(face_img):\n",
    "            return None\n",
    "            \n",
    "        # Enhance contrast using CLAHE\n",
    "        lab = cv2.cvtColor(face_img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        l = clahe.apply(l)\n",
    "        lab = cv2.merge((l,a,b))\n",
    "        face_img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        # Add padding to maintain aspect ratio\n",
    "        height, width = face_img.shape[:2]\n",
    "        aspect_ratio = width / height\n",
    "        \n",
    "        if aspect_ratio > 1:\n",
    "            new_width = target_size[0]\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "            padding_top = (target_size[1] - new_height) // 2\n",
    "            padding_bottom = target_size[1] - new_height - padding_top\n",
    "            padding_left = 0\n",
    "            padding_right = 0\n",
    "        else:\n",
    "            new_height = target_size[1]\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "            padding_left = (target_size[0] - new_width) // 2\n",
    "            padding_right = target_size[0] - new_width - padding_left\n",
    "            padding_top = 0\n",
    "            padding_bottom = 0\n",
    "            \n",
    "        face_img = cv2.resize(face_img, (new_width, new_height))\n",
    "        face_img = cv2.copyMakeBorder(\n",
    "            face_img, \n",
    "            padding_top, padding_bottom, padding_left, padding_right,\n",
    "            cv2.BORDER_CONSTANT, \n",
    "            value=[0, 0, 0]\n",
    "        )\n",
    "        \n",
    "        return face_img\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing face: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_faces_enhanced(images, labels, output_folder, batch_size=50):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Track statistics\n",
    "    total_images = 0\n",
    "    successful_detections = 0\n",
    "    failed_detections = 0\n",
    "    \n",
    "    for batch_start in range(0, len(images), batch_size):\n",
    "        batch_images = images[batch_start:batch_start + batch_size]\n",
    "        batch_labels = labels[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        for (filename, image), image_labels in zip(batch_images, batch_labels):\n",
    "            total_images += 1\n",
    "            print(f\"Processing {filename}...\")\n",
    "            \n",
    "            if image_labels == [\"nothing\"]:\n",
    "                continue\n",
    "                \n",
    "            # Detect faces with enhanced detection\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            face_boxes = detect_faces_enhanced(rgb_image)\n",
    "            print(face_boxes)\n",
    "            \n",
    "            if not face_boxes:\n",
    "                failed_detections += 1\n",
    "                print(f\"No faces detected in {filename}\")\n",
    "                continue\n",
    "                \n",
    "            successful_detections += 1\n",
    "            \n",
    "            # Process detected faces\n",
    "            for i, (box, label) in enumerate(zip(face_boxes, image_labels)):\n",
    "                x, y, w, h = box\n",
    "                \n",
    "                # Dynamic margin based on face size\n",
    "                margin = int(max(w, h) * 0.3)  # Increased margin\n",
    "                x = max(0, x - margin)\n",
    "                y = max(0, y - margin)\n",
    "                w = min(w + 2 * margin, image.shape[1] - x)\n",
    "                h = min(h + 2 * margin, image.shape[0] - y)\n",
    "                \n",
    "                face = image[y:y+h, x:x+w]\n",
    "                processed_face = preprocess_face_enhanced(face)\n",
    "                \n",
    "                if processed_face is not None:\n",
    "                    label_folder = os.path.join(output_folder, label.lower())\n",
    "                    os.makedirs(label_folder, exist_ok=True)\n",
    "                    face_path = os.path.join(label_folder, \n",
    "                                           f\"{os.path.splitext(filename)[0]}_face_{i}.jpg\")\n",
    "                    cv2.imwrite(face_path, cv2.cvtColor(processed_face, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "    print(f\"Successful detections: {successful_detections}\")\n",
    "    print(f\"Failed detections: {failed_detections}\")\n",
    "    print(f\"Success rate: {(successful_detections/total_images)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_folder, label_map=None):\n",
    "    images = []\n",
    "    image_labels = []\n",
    "\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            images.append((filename, img))\n",
    "            if label_map:\n",
    "                image_labels.append(label_map.get(filename, []))  # Default to empty list for test images\n",
    "\n",
    "    return images, image_labels if label_map else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training images...\n",
      "Processing 0032.jpg...\n",
      "[]\n",
      "No faces detected in 0032.jpg\n",
      "Processing 0195.jpg...\n",
      "[]\n",
      "No faces detected in 0195.jpg\n",
      "Processing 0569.jpg...\n",
      "[]\n",
      "No faces detected in 0569.jpg\n",
      "Processing 0601.jpg...\n",
      "[]\n",
      "No faces detected in 0601.jpg\n",
      "Processing 0072.jpg...\n",
      "[]\n",
      "No faces detected in 0072.jpg\n",
      "Processing 0703.jpg...\n",
      "[]\n",
      "No faces detected in 0703.jpg\n",
      "Processing 0516.jpg...\n",
      "[]\n",
      "No faces detected in 0516.jpg\n",
      "Processing 0387.jpg...\n",
      "[]\n",
      "No faces detected in 0387.jpg\n",
      "Processing 0710.jpg...\n",
      "[]\n",
      "No faces detected in 0710.jpg\n",
      "Processing 0427.jpg...\n",
      "[]\n",
      "No faces detected in 0427.jpg\n",
      "Processing 0356.jpg...\n",
      "[]\n",
      "No faces detected in 0356.jpg\n",
      "Processing 0463.jpg...\n",
      "[]\n",
      "No faces detected in 0463.jpg\n",
      "Processing 0325.jpg...\n",
      "[]\n",
      "No faces detected in 0325.jpg\n",
      "Processing 0235.jpg...\n",
      "[]\n",
      "No faces detected in 0235.jpg\n",
      "Processing 0151.jpg...\n",
      "[]\n",
      "No faces detected in 0151.jpg\n",
      "Processing 0281.jpg...\n",
      "[]\n",
      "No faces detected in 0281.jpg\n",
      "Processing 0158.jpg...\n",
      "[]\n",
      "No faces detected in 0158.jpg\n",
      "Processing 0590.jpg...\n",
      "[]\n",
      "No faces detected in 0590.jpg\n",
      "Processing 0159.jpg...\n",
      "[]\n",
      "No faces detected in 0159.jpg\n",
      "Processing 0155.jpg...\n",
      "[]\n",
      "No faces detected in 0155.jpg\n",
      "Processing 0290.jpg...\n",
      "[]\n",
      "No faces detected in 0290.jpg\n",
      "Processing 0135.jpg...\n",
      "[]\n",
      "No faces detected in 0135.jpg\n",
      "Processing 0190.jpg...\n",
      "[]\n",
      "No faces detected in 0190.jpg\n",
      "Processing 0626.jpg...\n",
      "[]\n",
      "No faces detected in 0626.jpg\n",
      "Processing 0367.jpg...\n",
      "[]\n",
      "No faces detected in 0367.jpg\n",
      "Processing 0042.jpg...\n",
      "[]\n",
      "No faces detected in 0042.jpg\n",
      "Processing 0794.jpg...\n",
      "[]\n",
      "No faces detected in 0794.jpg\n",
      "Processing 0130.jpg...\n",
      "[]\n",
      "No faces detected in 0130.jpg\n",
      "Processing 0068.jpg...\n",
      "[]\n",
      "No faces detected in 0068.jpg\n",
      "Processing 0046.jpg...\n",
      "[]\n",
      "No faces detected in 0046.jpg\n",
      "Processing 0616.jpg...\n",
      "[]\n",
      "No faces detected in 0616.jpg\n",
      "Processing 0600.jpg...\n",
      "[]\n",
      "No faces detected in 0600.jpg\n",
      "Processing 0545.jpg...\n",
      "[]\n",
      "No faces detected in 0545.jpg\n",
      "Processing 0497.jpg...\n",
      "[]\n",
      "No faces detected in 0497.jpg\n",
      "Processing 0389.jpg...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Crop and save faces with enhanced processing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mextract_and_save_faces_enhanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCropped faces have been saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mextract_and_save_faces_enhanced\u001b[0;34m(images, labels, output_folder, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Detect faces with enhanced detection\u001b[39;00m\n\u001b[1;32m     21\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 22\u001b[0m face_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces_enhanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(face_boxes)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m face_boxes:\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mdetect_faces_enhanced\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Enhanced face detection using multiple detection passes\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Direct RetinaFace detection without using detector variable\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     face_boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m     face_confidences \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/retinaface/RetinaFace.py:123\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m    121\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m im_tensor, im_info, im_scale \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39mpreprocess_image(img, allow_upscaling)\n\u001b[0;32m--> 123\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m net_out \u001b[38;5;241m=\u001b[39m [elt\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m net_out]\n\u001b[1;32m    125\u001b[0m sym_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/thomas_more/ai_frameworks/Face_Multiclassifier_Project/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "train_image_folder = \"../data/olda_data/cleaned_images\"\n",
    "label_csv_path = \"../data/labels/clean_data.csv\"\n",
    "output_folder = \"../data/faces_retinaface_enhanced/train_faces\"\n",
    "\n",
    "# Load label data\n",
    "import pandas as pd\n",
    "label_data = pd.read_csv(label_csv_path)\n",
    "label_data['label_name'] = label_data['label_name'].apply(eval)  # Convert string to list\n",
    "label_map = dict(zip(label_data['image'].astype(str).str.zfill(4) + \".jpg\", label_data['label_name']))\n",
    "\n",
    "# Load training images and labels\n",
    "train_images, train_labels = load_images(train_image_folder, label_map=label_map)\n",
    "\n",
    "# Crop and save faces with enhanced processing\n",
    "print(\"Processing training images...\")\n",
    "extract_and_save_faces_enhanced(train_images, labels=train_labels, output_folder=output_folder, batch_size=50)\n",
    "print(\"Cropped faces have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_test_faces_enhanced(images, output_folder, batch_size=50):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Track statistics\n",
    "    total_images = 0\n",
    "    successful_detections = 0\n",
    "    failed_detections = 0\n",
    "    \n",
    "    for batch_start in range(0, len(images), batch_size):\n",
    "        batch_images = images[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        for filename, image in batch_images:\n",
    "            total_images += 1\n",
    "            print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # Detect faces with enhanced detection\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            face_boxes = detect_faces_enhanced(rgb_image)\n",
    "            \n",
    "            if not face_boxes:\n",
    "                failed_detections += 1\n",
    "                print(f\"No faces detected in {filename}\")\n",
    "                continue\n",
    "                \n",
    "            successful_detections += 1\n",
    "            \n",
    "            # Process and save each face\n",
    "            for i, box in enumerate(face_boxes):\n",
    "                x, y, w, h = box\n",
    "                \n",
    "                # Dynamic margin based on face size\n",
    "                margin = int(max(w, h) * 0.3)\n",
    "                x = max(0, x - margin)\n",
    "                y = max(0, y - margin)\n",
    "                w = min(w + 2 * margin, image.shape[1] - x)\n",
    "                h = min(h + 2 * margin, image.shape[0] - y)\n",
    "                \n",
    "                face = image[y:y+h, x:x+w]\n",
    "                processed_face = preprocess_face_enhanced(face)\n",
    "                \n",
    "                if processed_face is not None:\n",
    "                    face_filename = f\"{os.path.splitext(filename)[0]}_face_{i}.jpg\"\n",
    "                    face_path = os.path.join(output_folder, face_filename)\n",
    "                    cv2.imwrite(face_path, cv2.cvtColor(processed_face, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "    print(f\"Successful detections: {successful_detections}\")\n",
    "    print(f\"Failed detections: {failed_detections}\")\n",
    "    print(f\"Success rate: {(successful_detections/total_images)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "test_image_folder = \"../data/images/test_images/cleaned_images\"\n",
    "test_output_folder = \"../data/faces_retinaface_enhanced/test_faces\"\n",
    "\n",
    "# Load test images\n",
    "test_images, _ = load_images(test_image_folder)\n",
    "\n",
    "# Crop and save test faces with enhanced processing\n",
    "print(\"Processing test images...\")\n",
    "extract_and_save_test_faces_enhanced(test_images, output_folder=test_output_folder, batch_size=50)\n",
    "print(\"Cropped faces from test set have been saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
